{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "##### Keywords: gradient descent,  pytorch, sgd, minibatch sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PyTorch\n",
    "\n",
    "Install pytorch by going to its web page\n",
    "\n",
    "\n",
    "#### Testing Installation\n",
    "\n",
    "If the code cell shows an error, then your PyTorch installation is not working and you should contact one of the teaching staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "0.2.1\n",
      "tensor([[0.7855, 0.5797, 0.4545],\n",
      "        [0.2648, 0.5096, 0.0966],\n",
      "        [0.0087, 0.1676, 0.7308],\n",
      "        [0.2209, 0.0587, 0.7217],\n",
      "        [0.5059, 0.1692, 0.7231]])\n",
      "-8.00169423514493\n"
     ]
    }
   ],
   "source": [
    "### Code Cell to Test PyTorch\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(torchvision.__version__)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "transforms.RandomRotation(0.7)\n",
    "transforms.RandomRotation([0.9, 0.2])\n",
    "\n",
    "t = transforms.RandomRotation(10)\n",
    "angle = t.get_params(t.degrees)\n",
    "\n",
    "print(angle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PyTorch?\n",
    "\n",
    "*All the quotes will come from the PyTorch About Page http://pytorch.org/about/ from which we'll plagiarize shamelessly.  After all, who better to tout the virtues of PyTorch than the creators?*\n",
    "\n",
    "\n",
    "### What is PyTorch?\n",
    "\n",
    "According to the PyTorch about page, \"PyTorch is a python package that provides two high-level features:\n",
    "\n",
    "- Tensor computation (like numpy) with strong GPU acceleration\n",
    "- Deep Neural Networks built on a tape-based autograd system\"\n",
    "\n",
    "### Why is it getting so popular?\n",
    "\n",
    "#### It's quite fast\n",
    "\n",
    "\"PyTorch has minimal framework overhead. We integrate acceleration libraries such as Intel MKL and NVIDIA (CuDNN, NCCL) to maximize speed. At the core, it’s CPU and GPU Tensor and Neural Network backends (TH, THC, THNN, THCUNN) are written as independent libraries with a C99 API.\n",
    "They are mature and have been tested for years.\n",
    "\n",
    "Hence, PyTorch is quite fast – whether you run small or large neural networks.\"\n",
    "\n",
    "#### Imperative programming experience\n",
    "\n",
    "\"PyTorch is designed to be intuitive, linear in thought and easy to use. When you execute a line of code, it gets executed. There isn’t an asynchronous view of the world. When you drop into a debugger, or receive error messages and stack traces, understanding them is straight-forward. The stack-trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\"\n",
    "\n",
    "\"PyTorch is not a Python binding into a monolothic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use numpy / scipy / scikit-learn etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython and Numba. Our goal is to not reinvent the wheel where appropriate.\"\n",
    "\n",
    "#### Takes advantage of GPUs easily\n",
    "\n",
    "\"PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerate compute by a huge amount.\n",
    "\n",
    "We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!\"\n",
    "\n",
    "\n",
    "#### Dynamic Graphs!!!\n",
    "\n",
    "\"Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world. One has to build a neural network, and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.\n",
    "\n",
    "With PyTorch, we use a technique called Reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as autograd, autograd, Chainer, etc.\n",
    "\n",
    "While this technique is not unique to PyTorch, it’s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with PyTorch Basics\n",
    "\n",
    "Enough of the sales pitch!  Let's start to understand the PyTorch basics.\n",
    "\n",
    "The basic unit of PyTorch is a tensor (basically a multi-dimensional array like a np.ndarray).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*_D5ZvufDS38WkhK9rK32hQ.jpeg)\n",
    "\n",
    "(image borrowed from https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32 )\n",
    "\n",
    "We can create PyTorch tensors directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.stefanfiott.com/machine-learning/tensors-and-gradients-in-pytorch/\n",
    "def tensor_properties(t, show_value=True):\n",
    "    print('Tensor properties:')\n",
    "    props = [('rank', t.dim()),\n",
    "             ('shape', t.size()),\n",
    "             ('data type', t.dtype),\n",
    "             ('tensor type', t.type())]\n",
    "    for s,v in props:\n",
    "        print('\\t{0:12}: {1}'.format(s,v))\n",
    "    if show_value:\n",
    "        #print('{0:12}: {1}'.format('value',t))\n",
    "        print(\"Value:\")\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 0\n",
      "\tshape       : torch.Size([])\n",
      "\tdata type   : torch.int64\n",
      "\ttensor type : torch.LongTensor\n",
      "Value:\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor always copies data. See below for 0-copy\n",
    "scalar = torch.tensor(5)\n",
    "tensor_properties(scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 1\n",
      "\tshape       : torch.Size([6])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "## You can create torch.Tensor objects by giving them data directly\n",
    "\n",
    "#  1D vector\n",
    "vector_input = [1., 2., 3., 4., 5., 6.]\n",
    "vector = torch.tensor(vector_input)\n",
    "tensor_properties(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([2, 3])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix\n",
    "matrix_input = [[1., 2., 3.], [4., 5., 6]]\n",
    "matrix = torch.tensor(matrix_input)\n",
    "tensor_properties(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 3\n",
      "\tshape       : torch.Size([2, 2, 2])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 3D tensor of size 2x2x2.\n",
    "tensor_input = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "tensor3d = torch.tensor(tensor_input)\n",
    "\n",
    "tensor_properties(tensor3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be created without any initialization or initialized with random data from uniform (rand()) or normal (randn()) distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([2, 5])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[ 0.0000e+00, -3.6893e+19, -3.7212e-21, -1.5849e+29,  2.8131e+20],\n",
      "        [ 1.7566e+25,  1.7748e+28,  0.0000e+00, -3.7237e-21,  8.5920e+09]])\n",
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([3, 5])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[ 0.0000e+00, -3.6893e+19,  0.0000e+00, -3.6893e+19,  4.2981e+21],\n",
      "        [ 6.3828e+28,  3.8016e-39,  0.0000e+00,  0.0000e+00, -3.6893e+19],\n",
      "        [ 0.0000e+00, -3.6893e+19,  2.8131e+20,  1.7566e+25,  1.7748e+28]])\n"
     ]
    }
   ],
   "source": [
    "# Tensors with no initialization\n",
    "x_1 = torch.Tensor(2, 5)\n",
    "y_1 = torch.Tensor(3, 5)\n",
    "tensor_properties(x_1)\n",
    "tensor_properties(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([5, 3])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[0.2612, 0.7302, 0.7795],\n",
      "        [0.9621, 0.2362, 0.3938],\n",
      "        [0.0813, 0.2182, 0.1140],\n",
      "        [0.7256, 0.3401, 0.7494],\n",
      "        [0.4888, 0.6210, 0.4900]])\n",
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([5, 5])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[0.8440, 0.6660, 0.4346, 0.3580, 0.3835],\n",
      "        [0.9997, 0.9853, 0.2406, 0.6101, 0.9860],\n",
      "        [0.9228, 0.9695, 0.3000, 0.4735, 0.2903],\n",
      "        [0.9267, 0.5977, 0.2435, 0.0379, 0.6333],\n",
      "        [0.4250, 0.6443, 0.6927, 0.4173, 0.5849]])\n"
     ]
    }
   ],
   "source": [
    "# Tensors initialized from uniform\n",
    "x_2 = torch.rand(5, 3)\n",
    "y_2 = torch.rand(5, 5)\n",
    "\n",
    "tensor_properties(x_2)\n",
    "tensor_properties(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([5, 3])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[-1.0954, -0.0614, -1.0212],\n",
      "        [ 1.0143, -0.0055, -1.2042],\n",
      "        [ 1.7418, -0.2050, -1.3398],\n",
      "        [ 1.2264,  0.9062,  0.5064],\n",
      "        [ 0.1187, -0.9342,  0.8012]])\n",
      "Tensor properties:\n",
      "\trank        : 2\n",
      "\tshape       : torch.Size([5, 5])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([[ 0.1948,  0.4439, -0.2306, -0.4700,  0.7891],\n",
      "        [-0.8612, -0.2490, -1.3734, -1.9507,  0.5581],\n",
      "        [ 0.0487, -0.7507, -1.0863,  0.5295, -0.1510],\n",
      "        [-0.2630, -1.1078,  1.4324,  0.2213, -0.0691],\n",
      "        [-0.0661,  2.2021, -0.1563, -0.6238,  0.3503]])\n"
     ]
    }
   ],
   "source": [
    "# Tensors initialized from normal\n",
    "x_3 = torch.randn(5, 3)\n",
    "y_3 = torch.randn(5, 5)\n",
    "\n",
    "tensor_properties(x_3)\n",
    "tensor_properties(y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected operations (arithmetic operations, addressing, etc) are all in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([[ 0.0000e+00, -3.6893e+19, -3.7212e-21, -1.5849e+29,  2.8131e+20],\n",
      "        [ 1.7566e+25,  1.7748e+28,  0.0000e+00, -3.7237e-21,  8.5920e+09]])\n",
      "tensor([[0.2612, 0.7302, 0.7795],\n",
      "        [0.9621, 0.2362, 0.3938],\n",
      "        [0.0813, 0.2182, 0.1140],\n",
      "        [0.7256, 0.3401, 0.7494],\n",
      "        [0.4888, 0.6210, 0.4900]])\n",
      "tensor([[-1.0954, -0.0614, -1.0212],\n",
      "        [ 1.0143, -0.0055, -1.2042],\n",
      "        [ 1.7418, -0.2050, -1.3398],\n",
      "        [ 1.2264,  0.9062,  0.5064],\n",
      "        [ 0.1187, -0.9342,  0.8012]])\n",
      "tensor([[-0.8342,  0.6687, -0.2417],\n",
      "        [ 1.9764,  0.2307, -0.8105],\n",
      "        [ 1.8231,  0.0133, -1.2258],\n",
      "        [ 1.9520,  1.2464,  1.2558],\n",
      "        [ 0.6075, -0.3132,  1.2913]])\n",
      "tensor([-1.0212, -1.2042, -1.3398,  0.5064,  0.8012])\n"
     ]
    }
   ],
   "source": [
    "# Expect (2,5)\n",
    "print(x_1.size())\n",
    "\n",
    "print(x_1)\n",
    "\n",
    "\n",
    "# Addition\n",
    "print(x_2)\n",
    "print(x_3)\n",
    "\n",
    "print(x_2 + x_3)\n",
    "\n",
    "# Addressing\n",
    "print(x_3[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to move between PyTorch and Numpy worlds with numpy() and torch.from_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -3.6893e+19, -3.7212e-21, -1.5849e+29,  2.8131e+20],\n",
      "        [ 1.7566e+25,  1.7748e+28,  0.0000e+00, -3.7237e-21,  8.5920e+09]])\n",
      "[[ 0.0000000e+00 -3.6893488e+19 -3.7212154e-21 -1.5849494e+29\n",
      "   2.8131290e+20]\n",
      " [ 1.7566070e+25  1.7748358e+28  0.0000000e+00 -3.7237446e-21\n",
      "   8.5920276e+09]]\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# PyTorch --> Numpy\n",
    "print(x_1)\n",
    "print(x_1.numpy())\n",
    "\n",
    "print(type(x_1))\n",
    "print(type(x_1.numpy()))\n",
    "\n",
    "numpy_x_1 = x_1.numpy()\n",
    "\n",
    "# does not makes a copy: just wraps a tensor object around the numpy array\n",
    "pytorch_x_1 = torch.from_numpy(numpy_x_1)\n",
    "\n",
    "print(type(numpy_x_1))\n",
    "print(type(pytorch_x_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally PyTorch provides some convenience mechanisms for concatenating Tensors via torch.cat() and reshaping them with  .view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([2, 8])\n",
      "tensor([[[-1.0053,  1.0082,  0.0516, -2.6949],\n",
      "         [ 2.1851,  2.2241,  1.0111,  0.6711],\n",
      "         [-1.5217, -0.4370, -0.0411,  2.3265]],\n",
      "\n",
      "        [[ 0.7362, -0.4631,  0.0565,  0.1602],\n",
      "         [-0.2661,  0.5695, -0.1632, -0.5906],\n",
      "         [ 1.2274, -0.3200, -0.3043, -0.9487]]])\n",
      "tensor([[-1.0053,  1.0082,  0.0516, -2.6949,  2.1851,  2.2241,  1.0111,  0.6711,\n",
      "         -1.5217, -0.4370, -0.0411,  2.3265],\n",
      "        [ 0.7362, -0.4631,  0.0565,  0.1602, -0.2661,  0.5695, -0.1632, -0.5906,\n",
      "          1.2274, -0.3200, -0.3043, -0.9487]])\n",
      "tensor([[-1.0053,  1.0082,  0.0516, -2.6949,  2.1851,  2.2241,  1.0111,  0.6711,\n",
      "         -1.5217, -0.4370, -0.0411,  2.3265],\n",
      "        [ 0.7362, -0.4631,  0.0565,  0.1602, -0.2661,  0.5695, -0.1632, -0.5906,\n",
      "          1.2274, -0.3200, -0.3043, -0.9487]])\n"
     ]
    }
   ],
   "source": [
    "## Concatenating\n",
    "\n",
    "# By default, it concatenates along the zeroth(first) axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1.shape)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2.shape)\n",
    "\n",
    "## Reshaping\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Variables and the Computational Graph\n",
    "\n",
    "Ok -- back to PyTorch.\n",
    "\n",
    "The other fundamental PyTorch construct besides Tensors are Variables.  Variables are very similar to tensors, but they also keep track of the graph (including their gradients for autodifferentiation).  They are defined in the autograd module of torch.\n",
    "\n",
    "This has changed in recent versions of pytorch, but i want to keep this section in as you will likely see code which uses `Variables`. A `Variable` bow is just a tensor with `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# Let's create a variable by initializing it with a tensor\n",
    "first_tensor = torch.Tensor([23.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 1\n",
      "\tshape       : torch.Size([1])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([23.3000])\n"
     ]
    }
   ],
   "source": [
    "tensor_properties(first_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_tensor.grad None\n"
     ]
    }
   ],
   "source": [
    "print(\"first_tensor.grad\", first_tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first variables gradient:  None\n",
      "first variables data:  tensor([23.3000])\n"
     ]
    }
   ],
   "source": [
    "first_variable = Variable(first_tensor, requires_grad=True)\n",
    "\n",
    "print(\"first variables gradient: \", first_variable.grad)\n",
    "print(\"first variables data: \", first_variable.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 1\n",
      "\tshape       : torch.Size([1])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([23.3000])\n"
     ]
    }
   ],
   "source": [
    "tensor_properties(first_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tensor_new = torch.tensor([23.3], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "\trank        : 1\n",
      "\tshape       : torch.Size([1])\n",
      "\tdata type   : torch.float32\n",
      "\ttensor type : torch.FloatTensor\n",
      "Value:\n",
      "tensor([23.3000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor_properties(first_tensor_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first variables gradient:  None\n",
      "first variables data:  tensor([23.3000])\n"
     ]
    }
   ],
   "source": [
    "print(\"first variables gradient: \", first_tensor_new.grad)\n",
    "print(\"first variables data: \", first_variable.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_tensor.grad None\n"
     ]
    }
   ],
   "source": [
    "print(\"first_tensor.grad\", first_tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some new variables. We can do so implicitly just by creating other variables with functional relationships to our variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data tensor([23.3000])\n",
      "z.grad:  None\n",
      "y.data:  tensor([11563.5557])\n",
      "y.grad:  None\n",
      "z.data:  tensor([1.])\n",
      "z.grad:  None\n",
      "x.grad: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = first_variable\n",
    "print(\"x.data\", x.data)\n",
    "y = (x * x) * (x - 2) # y is a variable\n",
    "z = torch.tanh(y) # z has a functional relationship to y, it's a variable\n",
    "print(\"z.grad: \", z.grad)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"y.data: \", y.data)\n",
    "print(\"y.grad: \", y.grad)\n",
    "\n",
    "print(\"z.data: \", z.data)\n",
    "print(\"z.grad: \", z.grad)\n",
    "\n",
    "print(\"x.grad:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data tensor([23.3000])\n",
      "z.grad:  None\n",
      "y.data:  tensor([1.5409e+33])\n",
      "y.grad:  None\n",
      "z.data:  tensor([1.])\n",
      "z.grad:  None\n",
      "x.grad: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = first_tensor_new\n",
    "print(\"x.data\", x.data)\n",
    "y = (x ** x) * (x - 2) # y is a variable\n",
    "z = torch.tanh(y) # z has a functional relationship to y\n",
    "print(\"z.grad: \", z.grad)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"y.data: \", y.data)\n",
    "print(\"y.grad: \", y.grad)\n",
    "\n",
    "print(\"z.data: \", z.data)\n",
    "print(\"z.grad: \", z.grad)\n",
    "\n",
    "print(\"x.grad:\", x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables (and now tensors requiring gradients) come with a .backward() that allows them to do autodifferentiation via backwards propagation.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
